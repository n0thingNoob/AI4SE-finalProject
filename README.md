# AI-Generated Trading Strategy Quality Assessment Project

A comprehensive framework for generating, testing, and evaluating AI-generated trading strategies using different LLM models (Gemini and GPT). This project includes automated code quality assessment, robustness testing, and comparative analysis between different AI models.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Testing Framework](#testing-framework)
- [Scoring System](#scoring-system)
- [Configuration](#configuration)
- [Contributing](#contributing)

## ğŸ¯ Overview

This project evaluates the quality and robustness of trading strategies automatically generated by AI/LLM models. It supports multiple AI models (currently Gemini and GPT) and provides:

- **Automated Strategy Generation**: AI models generate trading strategies based on market analysis
- **Comprehensive Testing**: Robustness tests with boundary conditions and random data
- **Code Quality Assessment**: Multi-dimensional scoring system for code quality
- **Model Comparison**: Side-by-side comparison of strategies generated by different AI models

## ğŸ“ Project Structure

```
AI4SE-finalProject/
â”œâ”€â”€ strategy/                          # Generated trading strategies
â”‚   â”œâ”€â”€ gemini/                        # Gemini-generated strategies
â”‚   â”‚   â”œâ”€â”€ analysis_json_output/      # Market analysis JSON files
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_nvda.json
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_tqqq.json
â”‚   â”‚   â”‚   â””â”€â”€ gemini_tsla.json
â”‚   â”‚   â””â”€â”€ generated_trading_code/    # Python strategy implementations
â”‚   â”‚       â”œâ”€â”€ gemini_nvda.py
â”‚   â”‚       â”œâ”€â”€ gemini_tqqq.py
â”‚   â”‚       â””â”€â”€ gemini_tsla.py
â”‚   â””â”€â”€ gpt/                           # GPT-generated strategies
â”‚       â”œâ”€â”€ analysis_json_output/      # Market analysis JSON files
â”‚       â”‚   â”œâ”€â”€ chatgpt_nvda.json
â”‚       â”‚   â”œâ”€â”€ chatgpt_tqqq.json
â”‚       â”‚   â””â”€â”€ chatgpt_tsla.json
â”‚       â””â”€â”€ generated_trading_code/    # Python strategy implementations
â”‚           â”œâ”€â”€ chatgpt_nvda.py
â”‚           â”œâ”€â”€ chatgpt_tqqq.py
â”‚           â””â”€â”€ chatgpt_tsla.py
â”œâ”€â”€ prompt/                            # LLM prompts for strategy generation
â”‚   â”œâ”€â”€ analysis_prompt.md             # Prompt for market analysis (JSON output)
â”‚   â””â”€â”€ code_generate_prompt.md        # Prompt for code generation (Python output)
â”œâ”€â”€ tests/                             # Testing framework
â”‚   â”œâ”€â”€ framework_stub.py              # Mock framework APIs for testing
â”‚   â”œâ”€â”€ strategy_loader.py             # Auto-discover and load strategies
â”‚   â”œâ”€â”€ test_data_generator.py         # Generate test data (boundary/random)
â”‚   â”œâ”€â”€ robustness_tests.py            # Robustness testing module
â”‚   â”œâ”€â”€ code_quality_scorer.py         # Code quality scoring system
â”‚   â”œâ”€â”€ test_generated_strategy.py     # Main test runner
â”‚   â”œâ”€â”€ run_tests.sh                   # Linux/Mac test script
â”‚   â”œâ”€â”€ run_tests.bat                  # Windows test script
â”‚   â””â”€â”€ README.md                      # Testing framework documentation
â”œâ”€â”€ pyproject.toml                     # Ruff/Pylint configuration
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .gitignore                         # Git ignore rules
â””â”€â”€ README.md                          # This file
```

## âœ¨ Features

### 1. LLM-Powered Strategy Generation
- **Standardized Prompts**: Reusable prompts for market analysis and code generation
- **Multi-Model Support**: Works with different AI models (Gemini, GPT)
- **Structured Output**: Market analysis in JSON, code in Python
- **Organized by Model**: Easy comparison between different AI models

### 2. Automated Testing Framework
- **Initialization Tests**: Verify strategy setup
- **Boundary Tests**: Test with None/NaN/Inf/extreme values
- **Random Tests**: Test with random market data
- **Auto-discovery**: Automatically finds and tests all strategies
- **Framework Stub**: Complete mock implementation for isolated testing

### 3. Code Quality Assessment
- **Robustness Score**: Based on test pass rates
- **Code Quality Score**: Multi-dimensional evaluation
  - Structure (20%)
  - Error Handling (25%)
  - Documentation (15%)
  - Complexity (15%)
  - Best Practices (25%)
- **Detailed Reports**: Individual file scores and aggregated statistics

### 4. Model Comparison
- Side-by-side comparison of different AI models
- Individual file scores and aggregated statistics
- Total scores with proper maximum values (100 Ã— file count)
- Average scores for each model

## ğŸš€ Installation

### Prerequisites
- Python 3.9 or higher
- pip package manager

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd AI4SE-finalProject
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“– Usage

### Workflow Overview

1. **Market Analysis**: Use `prompt/analysis_prompt.md` with LLM to generate market analysis JSON
2. **Code Generation**: Use `prompt/code_generate_prompt.md` with LLM to generate Python strategy code
3. **Testing**: Run automated tests to evaluate code quality and robustness
4. **Comparison**: Compare results across different AI models

### Running Tests

#### Method 1: Direct Python Script
```bash
python tests/test_generated_strategy.py
```

#### Method 2: Using pytest
```bash
pytest tests/test_generated_strategy.py -v
```

#### Method 3: Using Test Scripts

**Windows:**
```cmd
tests\run_tests.bat
```

**Linux/Mac:**
```bash
chmod +x tests/run_tests.sh
./tests/run_tests.sh
```

### Static Code Quality Checks

#### Ruff (Recommended)
```bash
ruff check strategy/ tests/
```

#### Bandit (Security Check, Optional)
```bash
bandit -r strategy/ -f json -o bandit-report.json
```

### Using Prompts

The `prompt/` directory contains standardized prompts for LLM interaction:

- **`analysis_prompt.md`**: Prompt for generating market analysis in JSON format
- **`code_generate_prompt.md`**: Prompt for generating Python trading strategy code

These prompts ensure consistent output format and API compliance across different LLM models.

## ğŸ§ª Testing Framework

The testing framework provides comprehensive evaluation of generated strategies:

### Test Types

1. **Initialization Test**: Verifies `initialize()` method executes correctly
2. **Boundary Tests**: Tests strategy behavior with:
   - `None` values
   - `NaN` (Not a Number)
   - `Inf` / `-Inf` (Infinity)
   - Extreme values (very large/small numbers)
   - Zero and negative values
3. **Random Tests**: Tests with randomly generated market data

### Framework Stub

The framework uses a `FrameworkStub` to mock trading framework APIs:
- `declare_strategy_type()`
- `declare_trig_symbol()`
- `current_price()`
- `ma()`, `rsi()` (technical indicators)
- `place_limit()`, `bid()`, `ask()` (order management)
- `position_holding_qty()`
- And more...

See `tests/framework_stub.py` for complete API list.

## ğŸ“Š Scoring System

### Robustness Score (0-100)
- Based on test pass rates
- Weighted: 60% boundary tests, 40% random tests
- Measures how well strategies handle edge cases

### Code Quality Score (0-100)
Evaluates five dimensions:

| Dimension | Weight | Description |
|-----------|--------|-------------|
| Structure | 20% | Code organization, required methods |
| Error Handling | 25% | Try-except blocks, None checks |
| Documentation | 15% | Docstrings and comments |
| Complexity | 15% | Cyclomatic complexity |
| Best Practices | 25% | Python conventions, code style |

### Overall Score
- Weighted average: **40% Robustness + 60% Code Quality**
- Grade assignment:
  - **A**: â‰¥90
  - **B**: â‰¥80
  - **C**: â‰¥70
  - **D**: â‰¥60
  - **F**: <60

### Model-Specific Statistics

The framework provides detailed statistics for each AI model:
- Individual file scores
- Average scores across all strategies
- Total scores (sum of all strategies, max = 100 Ã— file count)

## âš™ï¸ Configuration

### `pyproject.toml`
- Ruff linter configuration
- Pylint configuration (optional)
- Bandit security scan configuration (optional)

### `requirements.txt`
- Test dependencies (pytest)
- Static analysis tools (ruff, optional: pylint, bandit)

## ğŸ“ Strategy Code Structure

All generated strategies follow a standard interface based on the Moomoo Algo framework:

```python
class Strategy(StrategyBase):
    def initialize(self):
        declare_strategy_type(AlgoStrategyType.SECURITY)
        self.trigger_symbols()
        self.global_variables()
    
    def trigger_symbols(self):
        self.target_symbol = declare_trig_symbol()
    
    def global_variables(self):
        self.qty = show_variable(100, GlobalType.INT)
        # ... more parameters
    
    def custom_indicator(self):
        pass  # Required by framework
    
    def handle_data(self):
        # Main trading logic
        pass
```

### Key Framework APIs

- **Indicators**: `ma()`, `rsi()` for technical analysis
- **Market Data**: `current_price()`, `bid()`, `ask()`
- **Trading**: `place_limit()`, `position_holding_qty()`
- **Configuration**: `declare_strategy_type()`, `show_variable()`

See `prompt/code_generate_prompt.md` for complete API reference and usage guidelines.

## ğŸ” Example Output

```
MODEL-SPECIFIC SCORING SUMMARY
================================================================================

GEMINI Model:
  Total strategies: 3
  Passed: 3
  Failed: 0

  Individual File Scores:
    gemini_nvda.py:
      Overall: 76.60/100
      Robustness: 100.00/100
      Code Quality: 61.00/100
    ...

  Average Scores:
    Overall: 76.50/100
    Robustness: 100.00/100
    Code Quality: 60.83/100

  Total Scores (Sum of 3 strategies, max: 300.00):
    Overall: 229.50/300.00
    Robustness: 300.00/300.00
    Code Quality: 182.50/300.00
```

## ğŸ”„ Workflow

### Generating New Strategies

1. **Market Analysis**:
   - Use `prompt/analysis_prompt.md` with your LLM (Gemini/GPT)
   - Input: Stock ticker symbol
   - Output: JSON file with market analysis and strategy logic
   - Save to `strategy/{model}/analysis_json_output/`

2. **Code Generation**:
   - Use `prompt/code_generate_prompt.md` with your LLM
   - Input: JSON from step 1
   - Output: Python strategy code
   - Save to `strategy/{model}/generated_trading_code/`

3. **Testing**:
   - Run `python tests/test_generated_strategy.py`
   - Review scores and identify areas for improvement

4. **Iteration**:
   - Refine prompts based on test results
   - Regenerate strategies with improved prompts
   - Compare model performance

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `python tests/test_generated_strategy.py`
5. Ensure code quality: `ruff check .`
6. Submit a pull request

## ğŸ“„ License

[Add your license information here]

## ğŸ™ Acknowledgments

- Trading framework API design based on Moomoo Algo platform
- Test framework inspired by best practices in software testing
- LLM prompts designed for consistent, high-quality code generation

## ğŸ“š Additional Documentation

- **Testing Framework**: See [tests/README.md](tests/README.md) for detailed testing documentation
- **Code Generation Prompt**: See [prompt/code_generate_prompt.md](prompt/code_generate_prompt.md) for API reference
- **Analysis Prompt**: See [prompt/analysis_prompt.md](prompt/analysis_prompt.md) for market analysis guidelines

